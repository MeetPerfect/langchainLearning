from langchain.chat_models import init_chat_model
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

# description: langchainæ·»åŠ å¤šè½®å¯¹è¯è®°å¿†åŠŸèƒ½

prompt = ChatPromptTemplate.from_messages([
    SystemMessage(content="ä½ å«è‹äº•ç©ºï¼Œæ˜¯æ—¥æœ¬è‘—åå¥³æ¼”å‘˜ã€‚"),
    MessagesPlaceholder(variable_name="messages"),
])

model = init_chat_model(
    model="Qwen/Qwen3-8B",
    model_provider="openai",
    base_url="https://api.siliconflow.cn/v1/",
    api_key="sk-lzcunbanmnmklpxtfehbnmupbgytyqgujjulndjtvhzjhqdq",
)

chain = prompt | model | StrOutputParser()

messages_list = []
print("ğŸ”¹ è¾“å…¥ exit ç»“æŸå¯¹è¯")

while True:
    user_query = input("ç”¨æˆ·: ")

    if user_query.lower() in {"exit", "quit"}:
        print("å¯¹è¯ç»“æŸã€‚")
        break
    messages_list.append(HumanMessage(content=user_query))

    # æµå¼è¾“å‡º
    assistant_reply = ""
    for chunk in chain.stream({"messages": messages_list}):
        assistant_reply+=chunk
        print(chunk, end="", flush=True)
    print()  # æ¢è¡Œ

    messages_list.append(AIMessage(content=assistant_reply))

    messages_list = messages_list[-50:]
